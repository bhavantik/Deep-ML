{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transformation Matrix from Basis B to C\n",
    "- Deep-ML: https://www.deep-ml.com/problems/27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "- Given basis vectors in two different bases B and C for R^3, write a Python function to compute the transformation matrix P from basis B to C.\n",
    "\n",
    "### ðŸ§® Example\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "B = [[1, 0, 0], \n",
    "    [0, 1, 0], \n",
    "    [0, 0, 1]]\n",
    "C = [[1, 2.3, 3], \n",
    "    [4.4, 25, 6], \n",
    "    [7.4, 8, 9]]\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "[[-0.6772, -0.0126, 0.2342],\n",
    "                [-0.0184, 0.0505, -0.0275],\n",
    "                [0.5732, -0.0345, -0.0569]]\n",
    "```\n",
    "\n",
    "**Reasoning:**\n",
    "- The transformation matrix P from basis B to C can be found using matrix operations involving the inverse of matrix C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the about the topic\n",
    "\n",
    "Here's how you can format the explanation of **Transformation Matrices** in Markdown, suitable for pasting into a Jupyter Notebook:\n",
    "\n",
    "\n",
    "# Understanding Transformation Matrices\n",
    "\n",
    "A **transformation matrix** allows us to convert the coordinates of a vector in one basis to coordinates in another basis. For bases \\( B \\) and \\( C \\) of a vector space, the **transformation matrix** \\( P \\) from \\( B \\) to \\( C \\) is calculated as follows:\n",
    "\n",
    "## Steps to Calculate the Transformation Matrix\n",
    "\n",
    "1. **Inverse of Basis \\( C \\)**: First, find the inverse of the matrix representing basis \\( C \\), denoted \\( C^{-1} \\).\n",
    "   \n",
    "2. **Matrix Multiplication**: Multiply \\( C^{-1} \\) by the matrix of basis \\( B \\). The result is the transformation matrix:\n",
    "\n",
    "\\[\n",
    "P = C^{-1} \\cdot B\n",
    "\\]\n",
    "\n",
    "This matrix \\( P \\) can be used to transform any vector coordinates from the \\( B \\) basis to the \\( C \\) basis.\n",
    "`\n",
    "### Explanation:\n",
    "- **Basis \\( B \\) and \\( C \\)**: These are matrices that represent the coordinates of a vector in different bases.\n",
    "- **Transformation Matrix \\( P \\)**: This matrix is used to convert vector coordinates between the two bases. \n",
    "- **Inverse of \\( C \\)**: To compute the transformation matrix, we first need the inverse of the matrix representing basis \\( C \\), and then we multiply it by the matrix for basis \\( B \\).\n",
    "\n",
    "This process is essential in **coordinate transformations** in linear algebra, particularly when switching between different reference frames or vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.000000000000002, 4.000000000000002], [-4.000000000000002, -3.0000000000000018]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n",
    "    # Convert the lists to numpy arrays for easier matrix operations\n",
    "    B = np.array(B)\n",
    "    C = np.array(C)\n",
    "    \n",
    "    # Compute the inverse of matrix C\n",
    "    C_inv = np.linalg.inv(C)\n",
    "    \n",
    "    # Compute the transformation matrix P: P = C_inv * B\n",
    "    P = np.dot(C_inv, B)\n",
    "    \n",
    "    # Return the result as a list of lists\n",
    "    return P.tolist()\n",
    "\n",
    "# Example usage:\n",
    "B = [[1, 2], [3, 4]]\n",
    "C = [[5, 6], [7, 8]]\n",
    "\n",
    "output = transform_basis(B, C)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA\n",
    "\n",
    "## 1. Realworld Applications ?\n",
    "\n",
    "### Transformation Matrices in Machine Learning/AI: Real-World Use Case\n",
    "\n",
    "In machine learning and AI, transformation matrices are widely used for tasks like **coordinate transformations**, **feature scaling**, and **dimensionality reduction**. Let's explore how transformation matrices play a crucial role in these tasks, especially in the context of **changing the basis of a vector space** and how this concept is applied to real-world problems.\n",
    "\n",
    "### 1. **Changing the Basis in Machine Learning**:\n",
    "When we work with data in machine learning, the data might be represented in one set of coordinates (basis) but we might need to represent it in another set of coordinates. This is where transformation matrices come into play.\n",
    "\n",
    "#### Example: **Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a technique used to reduce the dimensionality of data while preserving as much variance as possible. It finds a new set of orthogonal axes (principal components) that capture the directions in which the data varies the most.\n",
    "\n",
    "- **Old Basis (Original Features)**: Suppose your data is in a 2D space represented by two features: **Height** and **Weight**. These features form the original basis.\n",
    "- **New Basis (Principal Components)**: PCA finds new axes (principal components) that represent the directions in which the data varies the most (e.g., combining Height and Weight into a new axis like **Body Mass**).\n",
    "\n",
    "The transformation matrix that PCA produces allows you to transform the original data points (in the old basis) into the new basis (the principal components). This process involves applying a **linear transformation** to the data using a transformation matrix.\n",
    "\n",
    "##### PCA Process:\n",
    "1. **Compute the Covariance Matrix**: The first step in PCA is to compute the covariance matrix, which measures how much the data points vary together.\n",
    "2. **Compute Eigenvalues and Eigenvectors**: Then, we compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the directions of maximum variance (principal components).\n",
    "3. **Construct Transformation Matrix**: The eigenvectors form the columns of the transformation matrix. By multiplying the original data matrix with this transformation matrix, we get the data in the new basis (principal components).\n",
    "\n",
    "### 2. **Use Case: Changing Coordinate Systems in Robotics**\n",
    "In robotics, particularly in **robot motion planning**, it's often required to transform coordinates from one reference frame to another. For example, a robot might have a camera attached to it. The position of the robot is in a world coordinate system, but the camera operates in a different coordinate system relative to the robot. \n",
    "\n",
    "By using transformation matrices, we can map the coordinates from the robot's coordinate system to the world coordinate system.\n",
    "\n",
    "#### Example:\n",
    "- The robotâ€™s position is described in the robot's own coordinate system.\n",
    "- The camera provides data in the camera's coordinate system.\n",
    "- To understand the camera's data in the world coordinates, we need a **transformation matrix** that converts the camera's coordinates to world coordinates.\n",
    "\n",
    "Hereâ€™s how the transformation works:\n",
    "1. **Matrix Representation**: You can represent the robot's position and orientation as a matrix. The world position and the camera's position can also be described by matrices.\n",
    "2. **Transformation**: You multiply the camera's coordinates by the transformation matrix, which converts the data into the world coordinates.\n",
    "\n",
    "This allows the robot to understand its environment in terms of a global coordinate system (the world) even though the data is collected in a local reference frame (camera or robot).\n",
    "\n",
    "### 3. **Use Case: Feature Scaling in Machine Learning**\n",
    "Another common use of transformation matrices is in **feature scaling**. Features in datasets often have different scales, meaning that some features may have larger numerical ranges than others. This can impact the performance of machine learning algorithms.\n",
    "\n",
    "One approach to solve this problem is to use **standardization**, where each feature is transformed into a new basis such that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "#### Example:\n",
    "If you have a dataset where one feature is in the range of `[1, 100]` (e.g., salary) and another feature is in the range `[0, 1]` (e.g., age), itâ€™s not ideal to feed these directly into machine learning models like **linear regression** or **neural networks**. These models might give more importance to the feature with the larger range, skewing the results.\n",
    "\n",
    "By applying **Z-score normalization** (standardization), you transform each feature to have the same scale. This is done by subtracting the mean and dividing by the standard deviation for each feature. The transformation can be represented by a matrix that converts the data from one basis to another (scaled basis).\n",
    "\n",
    "### Summary: Real-World Applications of Transformation Matrices\n",
    "1. **Principal Component Analysis (PCA)** for dimensionality reduction and feature transformation.\n",
    "2. **Robotics** for converting coordinates between different reference frames (e.g., robot's coordinate system and world coordinate system).\n",
    "3. **Feature Scaling** for machine learning to normalize features and improve the performance of algorithms.\n",
    "\n",
    "In these cases, transformation matrices help in **changing the basis** of the data, allowing us to operate on it in a way that enhances the effectiveness of our models and algorithms. Whether itâ€™s simplifying the data structure (PCA), improving data processing (robotics), or standardizing input features (machine learning), transformation matrices are a key mathematical tool in AI/ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

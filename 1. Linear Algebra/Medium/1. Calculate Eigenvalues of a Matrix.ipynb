{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Calculate Eigenvalues of a Matrix\n",
    "- Deep-ML: https://www.deep-ml.com/problems/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "- Write a Python function that calculates the eigenvalues of a 2x2 matrix. The function should return a list containing the eigenvalues, sort values from highest to lowest.\n",
    "\n",
    "### 🧮 Example\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "matrix = [[2, 1], [1, 2]]\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "[3.0, 1.0]\n",
    "```\n",
    "\n",
    "**Reasoning:**\n",
    "- ## Calculating Eigenvalues of a 2x2 Matrix\n",
    "\n",
    "The **eigenvalues** of a matrix are calculated using the **characteristic equation**.  \n",
    "For a 2×2 matrix **A**, the characteristic equation is:\n",
    "\n",
    "$$\n",
    "\\lambda^2 - \\text{trace}(A) \\cdot \\lambda + \\det(A) = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( \\lambda \\) are the **eigenvalues**  \n",
    "- \\( \\text{trace}(A) = a_{11} + a_{22} \\) (sum of the diagonal elements)  \n",
    "- \\( \\det(A) = a_{11}a_{22} - a_{12}a_{21} \\) (determinant of the matrix)\n",
    "\n",
    "Solving the quadratic equation above yields the two eigenvalues of matrix **A**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the about the topic\n",
    "\n",
    "## Calculate Eigenvalues\n",
    "\n",
    "Eigenvalues of a matrix offer significant insight into the matrix's behavior, particularly in the context of linear transformations and systems of linear equations.\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Definition\n",
    "\n",
    "For a square matrix **A**, eigenvalues are scalars \\( \\lambda \\) that satisfy the equation for some non-zero vector **v** (the eigenvector):\n",
    "\n",
    "$$\n",
    "A \\cdot v = \\lambda \\cdot v\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 Calculation for a 2x2 Matrix\n",
    "\n",
    "Given a 2x2 matrix:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The eigenvalues are determined by solving the **characteristic equation**:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "Which simplifies to the quadratic equation:\n",
    "\n",
    "$$\n",
    "\\lambda^2 - \\text{tr}(A) \\cdot \\lambda + \\det(A) = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\text{tr}(A) = a + d \\) is the **trace** of the matrix\n",
    "- \\( \\det(A) = ad - bc \\) is the **determinant** of the matrix\n",
    "\n",
    "Solving this quadratic yields the eigenvalues \\( \\lambda \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Significance\n",
    "\n",
    "Understanding eigenvalues is essential for analyzing the effects of linear transformations represented by the matrix. They play a key role in many areas such as:\n",
    "- Stability analysis\n",
    "- Vibration analysis\n",
    "- Principal Component Analysis (PCA) in machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eigenvalues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m eigenvalues\n\u001b[0;32m      5\u001b[0m matrix \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m calculate_eigenvalues(matrix)\n\u001b[0;32m      8\u001b[0m output\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mcalculate_eigenvalues\u001b[1;34m(matrix)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_eigenvalues\u001b[39m(matrix: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m|\u001b[39m\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m eigenvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eigenvalues' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n",
    "\ta,b = matrix[0]\n",
    "\tc,d = matrix[1]\n",
    "\n",
    "\ttrace = a + d\n",
    "\tdet = a*d - b*c\n",
    "\n",
    "    # Characteristic equation: λ² - trace * λ + det = 0\n",
    "\t# using equation to solve above equation a = b^2 -+ sqrt(b^2-4c) / 2a\n",
    "\t# D = (b^2-4c)\n",
    "\tdiscriminant =  trace**2 - 4*det\n",
    "\n",
    "\tif discriminant<0:\n",
    "\t\traise ValueError(\"Complex eigenvalues not supported in this implementation.\") \n",
    "\n",
    "\t# Calculate eigenvalues\n",
    "\tsqrt_discriminant = sqrt.math(discriminant)\n",
    "\tl1 = (trace+sqrt_discriminant)/2\n",
    "\tl2 = (trace - sqrt_discriminant) / 2\n",
    "\n",
    "\teigenvalues = sorted([l1,l2], reverse = True)\n",
    "\treturn eigenvalues\n",
    "\n",
    "matrix = [[2, 1], [1, 2]]\n",
    "\n",
    "output = calculate_eigenvalues(matrix)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA\n",
    "\n",
    "##  1. where can i use eigen value in machine learning\n",
    "\n",
    "Eigenvalues are super important in several **core concepts of machine learning**, especially in areas involving **linear algebra**, **dimensionality reduction**, and **optimization**. Here’s where and how you commonly use eigenvalues in ML:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 1. **Principal Component Analysis (PCA)**\n",
    "**Use case**: Dimensionality reduction\n",
    "\n",
    "- PCA finds **directions (principal components)** that maximize variance in data.\n",
    "- These directions are the **eigenvectors** of the **covariance matrix**, and the corresponding **eigenvalues** indicate how much variance is captured along each direction.\n",
    "  \n",
    "> Bigger eigenvalues = directions with more variance = more important components.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 2. **Linear Discriminant Analysis (LDA)**\n",
    "**Use case**: Supervised dimensionality reduction\n",
    "\n",
    "- LDA also involves computing a scatter matrix and solving an eigenvalue problem to find axes that best separate classes.\n",
    "  \n",
    "---\n",
    "\n",
    "### 🧮 3. **Covariance and Correlation Analysis**\n",
    "- Understanding the structure of the **covariance matrix** through its eigenvalues helps you detect multicollinearity and variance directions in data.\n",
    "- Tiny eigenvalues might indicate redundancy or near-linearly dependent features.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌀 4. **Spectral Clustering**\n",
    "**Use case**: Non-linear clustering\n",
    "\n",
    "- In spectral clustering, you build a **graph Laplacian**, then compute its **eigenvectors** (usually the smallest ones) to embed nodes into a lower-dimensional space for clustering (like with K-means).\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 5. **Optimization (e.g., in Neural Networks)**\n",
    "- **Hessian matrix** (second derivative of loss w.r.t. weights) is analyzed to understand curvature:\n",
    "  - If all **eigenvalues > 0**, it’s a local **minimum**\n",
    "  - If all < 0 → local **maximum**\n",
    "  - Mixed signs → **saddle point**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧬 6. **Autoencoders and Latent Spaces**\n",
    "- In some cases, understanding how **information is compressed** into latent space can be linked back to the **eigenvalues of weight matrices** or activation covariance matrices.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 TL;DR\n",
    "| Application                     | Use of Eigenvalues              |\n",
    "|-------------------------------|-------------------------------|\n",
    "| PCA                           | To measure importance of components |\n",
    "| LDA                           | To maximize class separability |\n",
    "| Covariance Analysis           | To understand variance directions |\n",
    "| Spectral Clustering           | To define cluster structure via Laplacian |\n",
    "| Neural Network Optimization   | To study curvature of the loss surface |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. what about 3*3 matrix?\n",
    "For a **3×3 matrix**, the eigenvalue calculation is more complex because the characteristic equation becomes a **cubic polynomial** (degree 3). Solving it analytically is possible but tedious. Instead, the easiest and most practical approach is to use **NumPy**, which efficiently computes eigenvalues of any square matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ NumPy-Based Eigenvalue Calculation (for 2x2, 3x3, or larger)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n",
    "    matrix_np = np.array(matrix)\n",
    "    eigenvalues = np.linalg.eigvals(matrix_np)\n",
    "    return sorted(eigenvalues, reverse=True)\n",
    "\n",
    "# Example usage for 3x3\n",
    "matrix = [\n",
    "    [4, 2, 1],\n",
    "    [0, 3, -1],\n",
    "    [0, 0, 2]\n",
    "]\n",
    "\n",
    "output = calculate_eigenvalues(matrix)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 🔢 Sample Output:\n",
    "```\n",
    "[4.0, 3.0, 2.0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Where This Is Used in Machine Learning:\n",
    "- **PCA (Principal Component Analysis)**: Eigenvalues determine how much variance each principal component explains.\n",
    "- **Covariance matrix analysis**: Eigenvalues tell you the \"strength\" or significance of directions (eigenvectors).\n",
    "- **Spectral Clustering**: Uses eigenvalues/eigenvectors of a graph Laplacian matrix.\n",
    "- **Optimization**: Hessian matrix eigenvalues help analyze the curvature of the loss function (convexity, saddle points).\n",
    "- **Deep Learning**: Used to analyze weight matrices' properties during training (e.g., in model compression).\n",
    "\n",
    "Let me know if you want to visualize eigenvectors too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are the Numpy commands used for eigenvalue & eigenvector?\n",
    "\n",
    "Here are all the **key NumPy commands** related to calculating **eigenvalues** and **eigenvectors** of a matrix:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 1. `np.linalg.eig()`\n",
    "Computes both **eigenvalues** and **eigenvectors** of a square matrix.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 1], [1, 2]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "```\n",
    "\n",
    "- `eigenvalues`: Array of eigenvalues.\n",
    "- `eigenvectors`: 2D array where each **column** is an eigenvector.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 2. `np.linalg.eigvals()`\n",
    "Returns **only the eigenvalues** of a matrix.\n",
    "\n",
    "```python\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 3. Sorting Eigenvalues (optional)\n",
    "You can sort eigenvalues from highest to lowest:\n",
    "\n",
    "```python\n",
    "sorted_vals = np.sort(eigenvalues)[::-1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 4. Normalize Eigenvectors (optional)\n",
    "Eigenvectors returned are already normalized to unit length, but if you compute them manually or want to ensure:\n",
    "\n",
    "```python\n",
    "normalized_vector = eigenvectors[:, 0] / np.linalg.norm(eigenvectors[:, 0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 5. Verify Eigen Decomposition\n",
    "Check if \\( A \\cdot v = \\lambda \\cdot v \\)\n",
    "\n",
    "```python\n",
    "idx = 0  # Choose eigenvalue/vector index\n",
    "λ = eigenvalues[idx]\n",
    "v = eigenvectors[:, idx]\n",
    "\n",
    "left = A @ v\n",
    "right = λ * v\n",
    "\n",
    "np.allclose(left, right)  # Returns True if they match\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Bonus: Use in PCA\n",
    "```python\n",
    "cov_matrix = np.cov(data, rowvar=False)\n",
    "eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "```\n",
    "\n",
    "This is common in **Principal Component Analysis** (PCA).\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a Jupyter Notebook-ready markdown version of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
